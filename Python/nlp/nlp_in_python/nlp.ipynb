{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73309f9c",
   "metadata": {},
   "source": [
    "## What is Natural Language Processing (NLP)?\n",
    "\n",
    "Human language is the primary way we communicate, but it is inherently complex, ambiguous, and unstructured. Computers, on the other hand, operate on structured, numerical data. Natural Language Processing (NLP) is the subfield of computer science and artificial intelligence that provides the methods and algorithms to bridge this gap. It enables computers to read, analyze, and derive meaning from human language in a smart and useful way.\n",
    "\n",
    "### The Standard NLP Workflow\n",
    "\n",
    "Most NLP projects follow a standard pipeline of steps to transform unstructured text into actionable insights:\n",
    "\n",
    "1.  **Raw Text**: The input to the pipeline, which can be any form of text data, such as a social media post, a legal document, a scientific paper, or a book.\n",
    "2.  **Preprocessing**: Cleaning and standardizing the raw text to prepare it for analysis. This involves removing noise and unnecessary elements to focus on the meaningful parts of the text. **Tokenization** is the first and most fundamental preprocessing step.\n",
    "3.  **Feature Extraction**: Converting the cleaned text into a numerical representation (vectors or matrices) that machine learning models can understand.\n",
    "4.  **Modeling**: Applying algorithms to the numerical features to perform a specific task, such as classifying text sentiment, translating between languages, summarizing long documents, or generating new content.\n",
    "\n",
    "### Tokenization: The First Step in Preprocessing\n",
    "\n",
    "**Tokenization** is the process of breaking down a continuous stream of text into smaller, meaningful units called **tokens**. These tokens serve as the basic building blocks for all further analysis. There are two primary levels of tokenization.\n",
    "\n",
    "#### Sentence Tokenization\n",
    "\n",
    "Sentence tokenization segments a block of text into its constituent sentences. Analyzing text at the sentence level is often more insightful than treating it as a single block, as it preserves the immediate context of words.\n",
    "\n",
    "The **Natural Language Toolkit (NLTK)** is a foundational library for NLP in Python that provides easy-to-use tools for these tasks.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "# The 'punkt' tokenizer models are required for tokenization.\n",
    "# You only need to download this once.\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text paragraph\n",
    "text_block = \"Natural Language Processing is a fascinating field. It allows us to build amazing applications. Shall we begin?\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = nltk.sent_tokenize(text_block)\n",
    "\n",
    "print(sentences)\n",
    "```\n",
    "\n",
    "#### Word Tokenization\n",
    "\n",
    "Word tokenization segments a sentence or text into its individual words and punctuation marks. This is an essential step for many downstream tasks, such as counting word frequencies, building a vocabulary, or identifying key terms in a document.\n",
    "\n",
    "```python\n",
    "# A single sentence for word tokenization\n",
    "sentence = \"Don't wait, claim your 100% free prize now!\"\n",
    "\n",
    "# Tokenize the sentence into words and punctuation\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "print(words)\n",
    "```\n",
    "\n",
    "As you can see, `word_tokenize` is intelligent enough to handle contractions like \"Don't\" by splitting it into \"Do\" and \"n't\", and it correctly separates all words and punctuation into distinct tokens. These tokenization steps are the gateway to nearly all other NLP techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57972706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jhonm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the punkt_tab package.\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d78e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'stock', 'market', 'saw', 'a', 'significant', 'dip', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "The stock market saw a significant dip today. Experts believe the downturn may continue.\n",
    "However, many investors are optimistic about future growth.\n",
    "\"\"\"\n",
    "# Tokenize the text into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Tokenize the first sentence you obtained into words\n",
    "words = nltk.word_tokenize(sentences[0])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f1a9e",
   "metadata": {},
   "source": [
    "## Handling Stop Words\n",
    "\n",
    "### What are Stop Words?\n",
    "\n",
    "**Stop words** are words that appear with extremely high frequency in a language but typically contribute little to the overall meaning of a text. They are the grammatical \"glue\" that holds sentences together, such as articles (\"a\", \"an\", \"the\"), prepositions (\"in\", \"on\", \"about\"), and conjunctions (\"and\", \"but\", \"or\").\n",
    "\n",
    "### The Rationale for Removal\n",
    "\n",
    "For many NLP tasks, the goal is to identify the core topics or themes of a document. In this context, stop words act as noise. By removing them, we achieve two primary benefits:\n",
    "\n",
    "1.  **Reduced Dimensionality**: We reduce the total number of unique words (the vocabulary size) that a model needs to consider.\n",
    "2.  **Increased Focus**: The model can focus on the content-bearing words that are more likely to be important for the task at hand.\n",
    "\n",
    "However, removing stop words is not always appropriate. For tasks that require understanding grammatical structure or nuanced meaning, such as machine translation or sentiment analysis on short texts, stop words can be essential and should be retained.\n",
    "\n",
    "### Implementation in NLTK\n",
    "\n",
    "The NLTK library provides pre-compiled lists of stop words for many languages. The process involves tokenizing the text and then filtering out any token that appears in the stop word list. It is crucial to convert tokens to a consistent case (typically lowercase) before checking for stop words.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the necessary NLTK data (only needs to be done once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the list of English stop words\n",
    "english_stop_words = set(stopwords.words('english')) # Use a set for faster lookups\n",
    "\n",
    "# Example of Removing Stop Words \n",
    "text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Filter out stop words using a list comprehension\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in english_stop_words]\n",
    "\n",
    "print(f\"Original Tokens: {tokens}\")\n",
    "print(f\"Filtered Tokens: {filtered_tokens}\")\n",
    "```\n",
    "\n",
    "### Handling Punctuation\n",
    "\n",
    "Punctuation marks are symbols used to structure language for human readability. For many NLP models that operate on a \"bag-of-words\" principle, these symbols provide no meaningful information and can cause issues by making the model treat \"word\" and \"word.\" as two distinct tokens.\n",
    "\n",
    "#### The Rationale for Removal\n",
    "\n",
    "Removing punctuation helps to standardize the text and reduce the vocabulary size. It is a common step in preparing text for tasks that focus on word frequency or keyword identification. Similar to stop words, punctuation should be retained for tasks that rely on full sentence structure or sentiment analysis, where a symbol like an exclamation mark can carry important meaning.\n",
    "\n",
    "#### Implementation in Python\n",
    "\n",
    "Python's built-in `string` module provides a convenient string containing common punctuation marks. We can filter our list of tokens to exclude any that are found in this string.\n",
    "\n",
    "```python\n",
    "import string\n",
    "\n",
    "# The string.punctuation constant contains common punctuation marks\n",
    "print(string.punctuation)\n",
    "\n",
    "# A Combined Workflow: Removing Stop Words and Punctuation \n",
    "text = \"This is a sample sentence, showing off the stop words filtration!\"\n",
    "\n",
    "# 1. Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# 2. Use a single list comprehension to remove stop words AND punctuation\n",
    "# We check if the lowercased word is in the stop words set OR if it is in the punctuation set.\n",
    "clean_tokens = [\n",
    "    word for word in tokens \n",
    "    if word.lower() not in english_stop_words and word not in string.punctuation\n",
    "]\n",
    "\n",
    "print(f\"Original Tokens: {tokens}\")\n",
    "print(f\"Clean Tokens (No Stop Words or Punctuation): {clean_tokens}\")\n",
    "```\n",
    "\n",
    "This combined workflow provides a simple yet effective method for cleaning raw text and preparing it for further feature extraction and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c756dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22993b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jhonm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['example', 'demonstrate', 'removing', 'stop', 'words']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the stopwords dataset from NLTK. This is required for filtering out stop words.\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load the list of English stop words. These are words that usually do not add significant meaning to text analysis.\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "text = \"This is an example to demonstrate removing stop words.\"\n",
    "\n",
    "# Tokenize the text into individual words and punctuation marks. Tokenization is a key first step in text preprocessing.\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stop words from the token list. This step helps focus analysis on the most meaningful words.\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Remove punctuation from the filtered tokens. Punctuation is often removed to standardize the vocabulary.\n",
    "clean_tokens = [word for word in filtered_tokens if word not in string.punctuation]\n",
    "\n",
    "display(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2494d33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reached', 'support', 'got', 'helpful', 'response', 'within', 'minutes', '!', '!', '!', '#', 'impressed']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "feedback = \"I reached out to support and got a helpful response within minutes!!! Very #impressed\"\n",
    "\n",
    "# Tokenize the provided feedback into words and punctuation. Tokenization is a key first step in text preprocessing.\n",
    "tokens = nltk.word_tokenize(feedback)\n",
    "\n",
    "# Get the list of English stopwords. These are words that usually do not add significant meaning to text analysis.\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# Remove English stop words from the token list. This step helps focus analysis on the most meaningful words.\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af8c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reached', 'support', 'got', 'helpful', 'response', 'within', 'minutes', 'impressed']\n"
     ]
    }
   ],
   "source": [
    "# Import the string module, which contains a list of common punctuation characters.\n",
    "import string\n",
    "\n",
    "# Clean the filtered_tokens list by removing all punctuation.\n",
    "# This step ensures that only meaningful words remain, further reducing noise in the data.\n",
    "clean_tokens = [word for word in filtered_tokens if word not in string.punctuation]\n",
    "\n",
    "print(clean_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fdb3ae",
   "metadata": {},
   "source": [
    "## What is Text Normalization?\n",
    "\n",
    "In natural language, the same concept can be expressed with many different word forms (e.g., \"run\", \"running\", \"ran\"). For a computer, these are all distinct strings. Text normalization is a crucial preprocessing step that groups these variations together. The goal is to reduce the vocabulary size (the number of unique tokens) and ensure that the core meaning of a word is treated consistently, which helps NLP models generalize better.\n",
    "\n",
    "### Lowercasing\n",
    "\n",
    "Lowercasing is the most basic and common normalization technique. Computers are case-sensitive, meaning they treat \"Data\", \"data\", and \"DATA\" as three different tokens. Converting all text to a single case (typically lowercase) resolves this.\n",
    "\n",
    "  * **Why**: It prevents the model from treating the same word with different capitalization as separate entities, which reduces vocabulary size and improves statistical analysis of word frequencies.\n",
    "  * **How**: Use the standard `.lower()` string method in Python.\n",
    "  * **When to Avoid**: Lowercasing should be avoided in tasks where case is meaningful, such as identifying proper nouns (e.g., distinguishing \"US\" the country from \"us\" the pronoun) or analyzing code.\n",
    "\n",
    "\n",
    "```python\n",
    "text = \"The DATA SCIENTIST used data from the Data Warehouse.\"\n",
    "lower_text = text.lower()\n",
    "print(lower_text)\n",
    "```\n",
    "\n",
    "### Reducing Words to Their Root Form\n",
    "\n",
    "Beyond casing, we often want to group words with the same core meaning, like \"run\", \"running\", and \"ran\". Stemming and lemmatization are two common techniques for this.\n",
    "\n",
    "#### Stemming\n",
    "\n",
    "**Stemming** is a heuristic process that reduces words to their \"stem\" or root form by crudely chopping off common prefixes and suffixes according to a set of rules.\n",
    "\n",
    "  * **Advantages**: It is computationally fast and simple.\n",
    "  * **Disadvantages**: The process often results in non-existent words (e.g., `organizations -> organizat`) because it does not consider the word's dictionary definition or context.\n",
    "  * **Implementation**: The **Porter Stemmer** is a classic and widely used stemming algorithm available in NLTK.\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokens = ['running', 'ran', 'computers', 'organization', 'finally']\n",
    "\n",
    "# Apply stemming to each token\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "print(f\"Original Tokens: {tokens}\")\n",
    "print(f\"Stemmed Tokens:  {stemmed_tokens}\")\n",
    "```\n",
    "\n",
    "#### Lemmatization\n",
    "\n",
    "**Lemmatization** is a more sophisticated process that reduces a word to its dictionary base form, known as the **lemma**. Unlike stemming, it considers the word's part of speech (POS) and meaning to produce a valid dictionary word.\n",
    "\n",
    "  * **Advantages**: The result is always a valid, interpretable word, which preserves more of the text's meaning.\n",
    "  * **Disadvantages**: It is significantly slower than stemming because it requires dictionary lookups (e.g., from a resource like WordNet).\n",
    "  * **Implementation**: The **WordNet Lemmatizer** in NLTK is a standard tool. A crucial detail is that the lemmatizer works best when the part of speech is provided. By default, it assumes the word is a noun.\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download the necessary NLTK data (only needs to be done once)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') # Open Multilingual Wordnet\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = ['running', 'ran', 'computers', 'organization', 'finally']\n",
    "\n",
    "# Apply lemmatization\n",
    "# Note: Lemmatizing 'ran' correctly to 'run' requires knowing it's a verb.\n",
    "# The default lemmatize() assumes it's a noun.\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word, pos='v') for word in tokens] # pos='v' for verb\n",
    "print(f\"Original Tokens:   {tokens}\")\n",
    "print(f\"Lemmatized Tokens: {lemmatized_tokens}\")\n",
    "```\n",
    "### Stemming vs. Lemmatization: A Comparison\n",
    "\n",
    "| Feature | Stemming | Lemmatization |\n",
    "| :--- | :--- | :--- |\n",
    "| **Process** | Crude heuristic (chops off endings) | Dictionary-based (considers meaning and POS) |\n",
    "| **Output** | Can be a non-word (e.g., `organizat`) | Always a valid dictionary word (e.g., `organization`)|\n",
    "| **Speed** | **Fast** | Slow |\n",
    "| **Accuracy** | Lower | **Higher** |\n",
    "| **Use Case** | Best for applications where speed is critical and interpretability is less important, such as search engine indexing. | Best for applications requiring grammatical accuracy and understanding of meaning, such as chatbots, machine translation, or sentiment analysis. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9305d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "data scientists and data engineers need data \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\" \n",
    "Data Scientists and data engineers need DATA \n",
    "\"\"\"\n",
    "\n",
    "lower_text = text.lower()\n",
    "print(lower_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00838b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jhonm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed token list: ['run', 'bat', 'organ', 'read']\n",
      "Lemmatized token list: ['running', 'bat', 'organization', 'reading']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download the WordNet lexical database, required for lemmatization.\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Initialize the PorterStemmer object for stemming.\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Initialize the WordNetLemmatizer object for lemmatization.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a list of tokens (words) to be normalized. These include different forms and pluralizations.\n",
    "tokens = [\"running\", \"bats\", \"organizations\", \"reading\"]\n",
    "\n",
    "# Apply stemming to each token. Stemming crudely removes suffixes to reduce words to their stems, which may not be valid words.\n",
    "stemmed = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Apply lemmatization to each token. Lemmatization uses a vocabulary and morphological analysis to return valid dictionary words.\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "print(f\"Stemmed token list: {stemmed}\")\n",
    "print(f\"Lemmatized token list: {lemmatized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3467076f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
