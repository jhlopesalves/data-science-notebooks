{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d54cb87",
   "metadata": {},
   "source": [
    "## What is Natural Language Processing (NLP)?\n",
    "\n",
    "Natural Language Processing (NLP) is a subfield of computer science and AI that sits at the intersection of linguistics and machine learning. Its primary goal is to bridge the gap between complex, unstructured human language and the structured, numerical world of computers. NLP incorporates statistics, machine learning, and deep learning models to enable computers to understand, analyze, and generate human language, intent, and sentiment from unstructured data like text and speech.\n",
    "\n",
    "### Key NLP Use Cases\n",
    "\n",
    "NLP powers a wide range of modern applications. Three prominent examples include:\n",
    "\n",
    "  * **Sentiment Analysis**: This is the use of NLP to interpret and classify the underlying subjective tone of a piece of text as positive, negative, or neutral[cite: 3]. It is widely used to analyze customer reviews, social media comments, and survey responses.\n",
    "\n",
    "  * **Named Entity Recognition (NER)**: NER is an information extraction task that locates and classifies named entities in unstructured text into pre-defined categories such as person names, organizations, locations, and dates[cite: 4]. For example, in the sentence \"John McCarthy was born on September 4, 1927,\" NER would identify \"John McCarthy\" as a `PERSON` and \"September 4, 1927\" as a `DATE`.\n",
    "\n",
    "  * **Text Generation**: This involves using language models to generate new, human-like text. Modern chatbots and large language models like ChatGPT are prime examples of this technology, which is trained on vast amounts of text data to learn patterns of language.\n",
    "\n",
    "### Introduction to spaCy\n",
    "\n",
    "**spaCy** is a free, open-source library for advanced NLP in Python. It is designed specifically for performance and is ideal for building production-ready systems for information extraction. Key features include:\n",
    "\n",
    "  * **Production-Ready**: Provides robust and fast components for real-world applications.\n",
    "  * **Comprehensive**: Supports a wide range of NLP tasks and over 64 languages.\n",
    "  * **Modern**: Implements state-of-the-art algorithms for various linguistic annotations.\n",
    "\n",
    "#### The spaCy Workflow: Processing Text\n",
    "\n",
    "The core of spaCy is the `nlp` object, which is created by loading a trained model. This object is a processing pipeline that takes raw text and converts it into a rich `Doc` object.\n",
    "\n",
    "  * **The `nlp` Object**: This is the central processing pipeline. You create it once by loading a model.\n",
    "  * **The `Doc` Object**: When you pass a string of text to the `nlp` object, it returns a `Doc` object. This is not just a string, but a sophisticated container that holds the processed text along with a wealth of linguistic annotations, including tokens, part-of-speech tags, and syntactic dependencies.\n",
    "\n",
    "#### Tokenization in spaCy\n",
    "\n",
    "**Tokenization** is the fundamental first step in any NLP pipeline: the process of breaking text into its smallest meaningful units, called **tokens**. spaCy's tokenizer is highly sophisticated and handles this process automatically when you create a `Doc` object.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# 1. Load the pre-trained English model\n",
    "nlp = spacy.load(\"en_core_web_sm\") # small model\n",
    "nlp = spacy.load(\"en_core_web_trf\") # large model\n",
    "\n",
    "# 2. Define the text to be processed\n",
    "text = \"A spaCy pipeline object is created.\"\n",
    "\n",
    "# 3. Process the text with the nlp object to create a Doc\n",
    "doc = nlp(text)\n",
    "\n",
    "# 4. Access the tokens by iterating through the Doc object\n",
    "# token.text provides the string representation of each token.\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "This simple process of creating a `Doc` object triggers a series of processing steps, with tokenization being the first. The resulting tokens serve as the foundation for all further linguistic analysis within the spaCy framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e287c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
