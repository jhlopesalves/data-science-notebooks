{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b99249",
   "metadata": {},
   "source": [
    "## An Introduction to Supervised Learning with scikit-learn\n",
    "\n",
    "Machine learning is broadly concerned with creating systems that learn from data to make predictions or uncover patterns. This field is typically divided into two main paradigms: unsupervised and supervised learning.\n",
    "\n",
    "  * **Unsupervised Learning**: This involves analyzing **unlabeled data** to discover hidden structures, patterns, or groupings. A classic example is customer segmentation, where a business might use clustering algorithms to group its customers into distinct categories based on purchasing behaviour, without any prior knowledge of those categories.\n",
    "\n",
    "  * **Supervised Learning**: In contrast, supervised learning works with **labeled data**. This means that for each data observation, we have a set of input features and a known, correct output value (the \"label\" or \"target\"). The primary goal is to train a model that learns the relationship between the features and the target, enabling it to accurately predict the target values for new, unseen data. The \"supervision\" comes from the fact that we provide the model with the correct answers during the training phase.\n",
    "\n",
    "### Core Concepts of Supervised Learning\n",
    "\n",
    "#### Classification vs. Regression\n",
    "\n",
    "Supervised learning problems can be further categorized into two types based on the nature of their target variable:\n",
    "\n",
    "1.  **Classification**: The target variable is **categorical**, meaning it consists of a finite set of discrete classes or labels. The model's goal is to predict which category a new observation belongs to.\n",
    "\n",
    "      * *Examples*: Predicting if an email is `spam` or `not spam`; classifying a tumour as `benign` or `malignant`; identifying a handwritten digit from `0` to `9`.\n",
    "\n",
    "2.  **Regression**: The target variable is **continuous**, meaning it can take on any numerical value within a given range. The model's goal is to predict a specific quantity.\n",
    "\n",
    "      * *Examples*: Predicting the price of a house based on its features (size, location); forecasting the temperature for tomorrow; estimating the total sales for the next quarter.\n",
    "\n",
    "#### Essential Terminology\n",
    "\n",
    "The field uses specific terminology that is crucial to understand:\n",
    "\n",
    "| Term | Synonyms | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **Features** | Predictor Variables, Independent Variables | The input variables (columns) used by the model to make a prediction. |\n",
    "| **Target Variable**| Dependent Variable, Response Variable| The output variable that we are trying to predict. |\n",
    "| **Sample/Observation**| Row, Instance | A single data point, consisting of a set of features and its corresponding target value. |\n",
    "| **Training Data** | Labeled Data | The dataset used to \"teach\" or `fit` the model. It contains both features and their known target values. |\n",
    "\n",
    "### Preparing Data for `scikit-learn`\n",
    "\n",
    "Before applying any supervised learning algorithm using `scikit-learn`, the data must meet several requirements:\n",
    "\n",
    "  * **No Missing Values**: Algorithms cannot process `NaN` (Not a Number) values. Any missing data must be either dropped or imputed (filled in with a plausible value).\n",
    "  * **Numeric Format**: The features and target variable must be in a numeric format. Categorical string data must be converted into numbers using techniques like one-hot encoding or label encoding.\n",
    "  * **Standard Data Structures**: The data should be stored in a `pandas` DataFrame or a `NumPy` array, as these are the primary data structures the library is designed to work with.\n",
    "\n",
    "It is also a critical best practice to perform **Exploratory Data Analysis (EDA)** before any modeling. EDA helps you understand the distributions, relationships, and potential issues within your data, which informs both data preparation and model selection.\n",
    "\n",
    "### The `scikit-learn` API: A Consistent Workflow\n",
    "\n",
    "One of the great strengths of `scikit-learn` is its simple and consistent API. The process for training and using most models follows the same four steps:\n",
    "\n",
    "1.  **Import**: Import the desired model class from the appropriate `sklearn` module.\n",
    "2.  **Instantiate**: Create an instance of the model class. This is where you can set **hyperparameters**â€”configurable parameters that control the model's learning process.\n",
    "3.  **Fit**: Train the model on your data by calling the `.fit(X, y)` method, where `X` is the array or DataFrame of features and `y` is the array of target labels. This is the \"learning\" step.\n",
    "4.  **Predict**: Once the model is fitted, make predictions on new, unseen data by calling the `.predict(X_new)` method.\n",
    "\n",
    "```python\n",
    "# A conceptual example of the scikit-learn syntax\n",
    "from sklearn.module import Model\n",
    "\n",
    "# 1. & 2. Instantiate the model\n",
    "model = Model(hyperparameter=value)\n",
    "\n",
    "# 3. Fit the model to training data\n",
    "# model.fit(X_features, y_target)\n",
    "\n",
    "# 4. Predict on new data\n",
    "# new_predictions = model.predict(X_new_features)\n",
    "```\n",
    "\n",
    "### A Practical Example: k-Nearest Neighbors (k-NN)\n",
    "\n",
    "To illustrate the concepts, let's examine a popular and intuitive classification algorithm: **k-Nearest Neighbors (k-NN)**.\n",
    "\n",
    "#### The k-NN Algorithm Explained\n",
    "\n",
    "The core idea of k-NN is simple: **to classify a new data point, look at the 'k' most similar data points (its \"nearest neighbors\") from the training data and assign the new point the label that appears most frequently among those neighbors (a \"majority vote\").**\n",
    "\n",
    "The \"closeness\" or \"similarity\" is typically measured using standard distance metrics, most commonly Euclidean distance. The `k` is a hyperparameter you choose; a small `k` makes the model sensitive to noise, while a large `k` can oversmooth the decision boundary.\n",
    "\n",
    "#### Implementing k-NN with `scikit-learn`\n",
    "\n",
    "Let's follow the standard `scikit-learn` workflow to build and use a k-NN classifier.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# --- 1. Prepare Data ---\n",
    "# Let's create a generic, reproducible dataset.\n",
    "# Imagine we are predicting if a component will fail (1) or not (0)\n",
    "# based on two sensor readings (pressure and vibration).\n",
    "np.random.seed(42)\n",
    "X_features = np.random.rand(100, 2) * 10\n",
    "y_target = np.random.randint(0, 2, 100)\n",
    "\n",
    "print(f\"Shape of feature matrix (X): {X_features.shape}\")\n",
    "print(f\"Shape of target vector (y): {y_target.shape}\")\n",
    "\n",
    "# --- 2. Import and Instantiate the Model ---\n",
    "# We'll choose k=5 neighbors for this example.\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# --- 3. Fit the Model to the Data ---\n",
    "# The model \"learns\" by storing the training data.\n",
    "knn.fit(X_features, y_target)\n",
    "print(\"\\nModel has been fitted.\")\n",
    "\n",
    "# --- 4. Predict on Unlabeled Data ---\n",
    "# Let's create some new, unseen data points to make predictions for.\n",
    "X_new_data = np.array([\n",
    "    [2.5, 4.8],  # First new component\n",
    "    [8.1, 7.3],  # Second new component\n",
    "    [1.9, 1.1]   # Third new component\n",
    "])\n",
    "\n",
    "print(f\"\\nShape of new data matrix: {X_new_data.shape}\")\n",
    "\n",
    "# Use the fitted model to predict the labels for the new data\n",
    "predictions = knn.predict(X_new_data)\n",
    "\n",
    "print(f\"\\nPredictions for new data: {predictions}\")\n",
    "# The output will be an array like [0, 1, 0], predicting the class for each new sample.\n",
    "```\n",
    "\n",
    "This example encapsulates the entire supervised learning process: preparing labeled data (`X_features`, `y_target`), training a model (`knn.fit`), and using it to predict outcomes for new, unlabeled data (`knn.predict`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fe8954",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
